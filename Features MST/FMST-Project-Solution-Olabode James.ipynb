{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Model Selection and Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developed by: Olabode James"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Context: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the concrete strength using the data available in file concrete_data.xls. Apply\n",
    "feature engineering and model tuning to obtain 80% to 95% of R2score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "#Visualization Components\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # matplotlib.pyplot plots data\n",
    "sns.set(color_codes=True) # adds a nice background to the graphs\n",
    "# In order to enable plotting graphs in Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#Cross validation and data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "#ML Models For use \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Features Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "#Regression metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "#Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Standard maths\n",
    "import math\n",
    "\n",
    "#Features preProcessing\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Handling Statistical Components\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Need to handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData = pd.read_csv('concrete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no null values - implies we have relatively clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.applymap(np.isreal).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also, all entries are real - numbers, which implies we have relatively clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Exploratory data quality report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Univariate analysis â€“ data types and description of the independent attributes which should include (name, meaning, range of values observed, central values (mean and median), standard deviation and quartiles, analysis of the body of distributions / tails, missing values, outliers (10 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further confirmation of check for missing values - null \n",
    "round(concData.isna().sum()*100/concData.shape[0],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the datatypes \n",
    "concData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "pos = 1\n",
    "for i in concData.columns:\n",
    "    plt.subplot(3, 3, pos)\n",
    "    sns.boxplot(concData[i])\n",
    "    pos += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: There are outliers, rather than handling those separately here - we will apply RobustScaler later in the data handling before application of ML estimator/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "posHist = 1\n",
    "for i in concData.columns:\n",
    "    plt.subplot(3, 3, posHist)\n",
    "    plt.hist(concData[i])\n",
    "    plt.xlabel(i)\n",
    "    posHist += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bi-variate analysis between the predictor variables and between the predictor variables and target column. Comment on your findings in terms of their relationship and degree of relation if any. Visualize the analysis using boxplots and pair plots, histograms or density curves. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concData.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for multi-collinearity between features\n",
    "# Lets check for highly correlated variables\n",
    "cor= concData.corr()\n",
    "cor.loc[:,:] = np.tril(cor,k=-1)\n",
    "cor=cor.stack()\n",
    "cor[(cor > 0.8) | (cor< -0.8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Empty series - implies we will not need to worry about multicollinearity between feature variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(concData, palette=\"husl\", diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering techniques (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let do Train-validation-Test split - before refinement - \n",
    "# Goal will be exploring the opportunities of Features Engineering, to see extra performance improvement which is \n",
    "#obtainable while keeping two versions of the dataset - while paying attention to prevent using test data to validate\n",
    "\n",
    "# independant variables\n",
    "X = concData.drop(['strength'], axis=1)\n",
    "\n",
    "# the dependent variable\n",
    "y = concData[['strength']]\n",
    "\n",
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lin Reg  to use in feature selection\n",
    "linR = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building feature selection process using - SequentialFeatureSelector, to determine feature relevances\n",
    "sfs1 = sfs(linR, k_features=5, forward=True, scoring='r2', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SFFS\n",
    "sfs1 = sfs1.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.get_metric_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "fig = plot_sfs(sfs1.get_metric_dict())\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. R^2)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features have high predictive relevance\n",
    "columnList = list(X_train.columns)\n",
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "subsetColumnList = [columnList[i] for i in feat_cols] \n",
    "print(subsetColumnList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For comparison, we check performance difference on selected features and entire features set to see what \n",
    "#insights we can draw\n",
    "ml_model = 'Linear Regression'\n",
    "features_used = 'selected'\n",
    "linR = LinearRegression()\n",
    "linR.fit(X_train[subsetColumnList], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = linR.predict(X_train[subsetColumnList])\n",
    "train_score = linR.score(X_train[subsetColumnList], y_train)\n",
    "print('Training accuracy on selected features: %.3f' % train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = linR.predict(X_test[subsetColumnList])\n",
    "test_score = linR.score(X_test[subsetColumnList], y_test)\n",
    "print('Testing accuracy on selected features: %.3f' % test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresPerfDF = pd.DataFrame({'Model' : [ml_model], 'Features' : [features_used], 'Training R2 Score' : [train_score],\n",
    "                      'Test R2 Score' : [test_score]})\n",
    "featuresPerfDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance on Full feature set\n",
    "\n",
    "ml_model = 'Linear Regression'\n",
    "features_used = 'All'\n",
    "\n",
    "linR = LinearRegression()\n",
    "linR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = linR.predict(X_train)\n",
    "train_score = linR.score(X_train, y_train)\n",
    "print('Training accuracy on selected features: %.3f' % train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = linR.predict(X_test)\n",
    "test_score = linR.score(X_test, y_test)\n",
    "print('Testing accuracy on selected features: %.3f' % test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresPerfDF.loc[1] = [ml_model, features_used, train_score, test_score]\n",
    "featuresPerfDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: We obtained higher R2 Score when we use all the features to build our model, while using selected features has computational performance advantage - its influence was lacking in predictive power in reducing prediction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We desire a scaler that will handle presence of outliers while standardizing the dataset for the estimator \n",
    "#- from documentation, RobustScaler does that best\n",
    "rb_scaler = RobustScaler(quantile_range=(25, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering using Polynomial Features - So see if any improvement on Linear Regression - this will be used\n",
    "# for comparison later\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "X_train2 = poly.fit_transform(X_train)\n",
    "X_test2 = poly.fit_transform(X_test)\n",
    "X_train2.shape, X_test2.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Creating the model and tuning it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Algorithms that you think will be suitable for this project (at least 3 algorithms). Use Kfold Cross Validation to evaluate model performance. Use appropriate metrics and make a DataFrame to compare models w.r.t their metrics. (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_regression_metrics(true_data, pred_data):\n",
    "    mae = mean_absolute_error(true_data, pred_data)\n",
    "    mse = mean_squared_error(true_data, pred_data)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2score = r2_score(true_data, pred_data)  \n",
    "    return mae, mse, rmse, r2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_regression_plotter(true_data, pred_data):\n",
    "    # Let's visualize the model against the test data\n",
    "    plt.figure(figsize=(20,10))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(true_data, pred_data, edgecolors=(0, 0, 0))\n",
    "    ax.plot([true_data.min(), true_data.max()], [true_data.min(), true_data.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(\"Actual Concrete Strength vs Predicted\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making pipeline object for all the estimators that will be used\n",
    "#1. LinearRegression()\n",
    "#2. LinearRegression() with Polynomial Features\n",
    "#3. LinearRegression() - comparison on Lasso and Ridge Regression\n",
    "#4. DecisionTree Regressor - max_depth=none\n",
    "#5. DecisionTree Regressor - prune, max_depth=5\n",
    "#6. RandomForest Regressor \n",
    "#7. SupportVector Regressor\n",
    "#8. GradientBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the KFold for Cross validation to evaluate model performance\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = 'Linear Regression'\n",
    "indexer = 0\n",
    "\n",
    "pipe_lr = make_pipeline(rb_scaler, LinearRegression())\n",
    "results = cross_val_score(pipe_lr, X, y, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred= pipe_lr.predict(X_test)\n",
    "\n",
    "#y_pred = cross_val_predict(pipe_lr, X, y, cv=kfold)\n",
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = pd.DataFrame({'Model' : [ml_model], 'MAE' : [mae], 'MSE' : [mse],\n",
    "                      'RMSE' : [rmse], 'R2 Score' : [r2score], 'CV_Score_Avg': [cv_avg], 'CV_Score_STD': [cv_std]})\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Score average and 95 percent confidence interval: %0.5f (+/- %0.5f)\" % (results.mean(),  results.std()* 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if Poly features extraction will improve our results any better \n",
    "ml_model = 'Linear Regression - PolyFeatures'\n",
    "indexer += 1\n",
    "\n",
    "pipe_lr_poly = make_pipeline(rb_scaler, linear_model.LinearRegression())\n",
    "results = cross_val_score(pipe_lr_poly, X, y, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()\n",
    "\n",
    "pipe_lr_poly.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = pipe_lr_poly.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)\n",
    "\n",
    "resultsDF.loc[indexer] = [ml_model, mae, mse, rmse, r2score, cv_avg, cv_std]\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Score average and 95 percent confidence interval: %0.5f (+/- %0.5f)\" % (results.mean(),  results.std()* 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Polynomial Features (with only interaction terms) have improved the Out of sample R^2, MSE and other metrics. However at the cost of increaing the number of variables significantly from 8 to 37.\n",
    "\n",
    "With the general improvement in the R2 Score, however R2 Score generally improves with increase in the number of \n",
    "Features - we can thus not effectively conclude the model is better than the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using DecisionTreeRegressor\n",
    "# create a regressor object \n",
    "\n",
    "ml_model = 'Decision Tree Regressor'\n",
    "indexer += 1\n",
    "\n",
    "pipe_dt_rgr = make_pipeline(rb_scaler, DecisionTreeRegressor(random_state = seed))\n",
    "results = cross_val_score(pipe_dt_rgr, X, y, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dt_rgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred= pipe_dt_rgr.predict(X_test)\n",
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF.loc[indexer] = [ml_model, mae, mse, rmse, r2score, cv_avg, cv_std]\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Score average and 95 percent confidence interval: %0.5f (+/- %0.5f)\" % (results.mean(),  results.std()* 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Decision Tree Regression with no restriction on max_depth, gave higher and better, coefficient of determination R^2 , Model score of the prediction than both forms of Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if improvement is possible with Decision Tree Regressor, when Max_depth is limited to 5\n",
    "\n",
    "ml_model = 'Decision TreeR(pruned,max_depth=5)'\n",
    "indexer += 1\n",
    "\n",
    "pipe_dt_rgr_pruned = make_pipeline(rb_scaler, DecisionTreeRegressor(max_depth=5, random_state = seed))\n",
    "results = cross_val_score(pipe_dt_rgr_pruned, X, y, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_dt_rgr_pruned.fit(X_train, y_train)\n",
    "\n",
    "y_pred= pipe_dt_rgr_pruned.predict(X_test)\n",
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF.loc[indexer] = [ml_model, mae, mse, rmse, r2score, cv_avg, cv_std]\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model average Score and 95 percent confidence interval: %0.5f (+/- %0.5f)\" % (results.mean(),  results.std()* 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Decision Tree Regressor with Max_depth 5 performed worse than unpruned Decision Tree Regressor, opportunity for hyperparamter tuning exist here to get best parameters of the model. \n",
    "\n",
    "It is important to examine the features of importance in the decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression_plotter(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importances = pipe_dt_rgr.feature_importances_\n",
    "#print(\"Important Features: \" importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing RandomForest Regressor\n",
    "\n",
    "ml_model = 'RandomForest Regressor'\n",
    "indexer += 1\n",
    "\n",
    "pipe_rf_rgr = make_pipeline(rb_scaler, RandomForestRegressor(random_state=seed))\n",
    "results = cross_val_score(pipe_rf_rgr, X, y, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf_rgr.fit(X_train, y_train)\n",
    "\n",
    "y_pred= pipe_rf_rgr.predict(X_test)\n",
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF.loc[indexer] = [ml_model, mae, mse, rmse, r2score, cv_avg, cv_std]\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model average Score and 95 percent confidence interval: %0.5f (+/- %0.5f)\" % (results.mean(),  results.std()* 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: RandomForest Regressor has given best the best prediction, followed by unpruned Decision Tree - we will examine if this performance will be exceeded by SVR and GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing SupportVector Regressor\n",
    "\n",
    "ml_model = 'SupportVector Regressor'\n",
    "indexer += 1\n",
    "\n",
    "pipe_svr = make_pipeline(rb_scaler, SVR(C=1.0, epsilon=0.2))\n",
    "results = cross_val_score(pipe_svr, X, y, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svr.fit(X_train, y_train)\n",
    "\n",
    "y_pred= pipe_svr.predict(X_test)\n",
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF.loc[indexer] = [ml_model, mae, mse, rmse, r2score, cv_avg, cv_std]\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model average Score and 95 percent confidence interval: %0.5f (+/- %0.5f)\" % (results.mean(),  results.std()* 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing GradientBoost Regressor\n",
    "\n",
    "ml_model = 'GradientBoost Regressor'\n",
    "indexer += 1\n",
    "\n",
    "pipe_gdr = make_pipeline(rb_scaler, GradientBoostingRegressor(random_state=seed))\n",
    "results = cross_val_score(pipe_gdr, X, y, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_gdr.fit(X_train, y_train)\n",
    "\n",
    "y_pred= pipe_gdr.predict(X_test)\n",
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF.loc[indexer] = [ml_model, mae, mse, rmse, r2score, cv_avg, cv_std]\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model average Score and 95 percent confidence interval: %0.5f (+/- %0.5f)\" % (results.mean(),  results.std()* 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression_plotter(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Gradient boost Regressor gave the best R2 Score, coefficient of determination of all the models used. Also, the least Root Mean Square Error as well as Mean Square Error. It is followed by RandomForest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ Iteration 2 on Linear Regression ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can Lasso or Ridge Regression be used to improve the results from the best LinearRegression Model derived \n",
    "#from Polynomial Features?\n",
    "\n",
    "ml_model = 'Lasso Regression - Poly'\n",
    "indexer += 1\n",
    "\n",
    "lasso = Lasso(alpha=0.01)\n",
    "\n",
    "pipe_lr_poly_lasso = make_pipeline(rb_scaler, Lasso(alpha=0.01))\n",
    "results = cross_val_score(pipe_lr_poly_lasso, X_train2, y_train, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr_poly_lasso.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = pipe_lr_poly_lasso.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF.loc[indexer] = [ml_model, mae, mse, rmse, r2score, cv_avg, cv_std]\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model average Score and 95 percent confidence interval: %0.5f (+/- %0.5f)\" % (results.mean(),  results.std()* 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSIGHT: Lasso on PolyFeatures is a better improvement on performance of the LinearRegression -This implies the features combination has better predictive capability than otherwise. Thus, we need to find the features with higher contribution to concrete compressive strength within the mix of n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = 'Ridge Regression - Poly'\n",
    "indexer += 1\n",
    "\n",
    "pipe_lr_poly_ridge = make_pipeline(rb_scaler, Ridge(alpha=.3))\n",
    "results = cross_val_score(pipe_lr_poly_ridge, X_train2, y_train, cv=kfold)\n",
    "\n",
    "cv_avg = np.mean(abs(results))\n",
    "cv_std = results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr_poly_ridge.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = pipe_lr_poly_ridge.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae, mse, rmse, r2score = model_regression_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF.loc[indexer] = [ml_model, mae, mse, rmse, r2score, cv_avg, cv_std]\n",
    "resultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting Algorithms based on performance on Cross validation scores, \n",
    "#which is highly correlated with R2Score on Test data\n",
    "resultsDF.sort_values(by='CV_Score_Avg', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSIGHT: RandomForest Regressor and GradientBoost Regressor had better performance with 93 and 92.7% performance on Cross validation respectively - we will thus apply next stage of hyperparameter tuning to see if further improvement will be obtainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Techniques employed to squeeze that extra performance out of the model without making it over fit. Use Grid Search or Random Search on any of the two models used above. Make a DataFrame to compare models after hyperparameter tuning and their metrics as above. (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation set will be 75:25 of train set, which contains larger data - this is necessary so we don't \n",
    "#tune hyperparameters on seen data - information leak, but on unseen data - validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using GridSearchCV on best two models above - to examine if extra performance can be obtained \n",
    "#1. RandomForestRegressor\n",
    "#2. GradientBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTunedModelfit(ml_alg, performCV=True, printFeatureImportance=True, cv_folds=num_folds):\n",
    "    \n",
    "    #Fit the best algorithm on the data not seen\n",
    "    ml_alg.fit(X_val, y_val)\n",
    "        \n",
    "    #Predict training set, test the model:\n",
    "    dtrain_predictions = ml_alg.predict(X_test)\n",
    "    mae, mse, rmse, r2score = model_regression_metrics(y_test, dtrain_predictions)\n",
    "   \n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(ml_alg, X_val, y_val, cv=cv_folds)\n",
    "    \n",
    "    #Print model report:\n",
    "    print ( \"\\nModel Report\")\n",
    "    print (\"R2 Score : %.4g\" %  r2score)\n",
    "    print (\"MSE: %f\" % mse)\n",
    "    \n",
    "    if performCV:\n",
    "        print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))) \n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    predictors = X_val.columns\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(ml_alg.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "    return mae, mse, rmse, r2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to performance constraints - will limit number and scope of hyper parameters to tune\n",
    "#RandomForest Regression\n",
    "#Desired Hyperparamters \n",
    "\n",
    "# \n",
    "#\"n_estimators\":[100,200,300],\n",
    "# \"max_depth\": [3, None],\n",
    "#\"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "#\"min_samples_split\": [2, 3, 10],\n",
    "#\"min_samples_leaf\": [1, 3, 10],\n",
    "#\"bootstrap\": [True, False],\n",
    "#\"criterion\": [\"mse\", \"mae\"]\n",
    "#\n",
    "\n",
    "# RandomForest Regressor - Hyperparameters use a full grid over all parameters\n",
    "rfr_param_grid = {\"n_estimators\":[100,200],\n",
    "                  \"max_depth\": [3, None],\n",
    "                  \"max_features\": [\"auto\", \"sqrt\"],\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"mse\", \"mae\"]}\n",
    "\n",
    "\n",
    "#\n",
    "#\"max_depth\": [3, None],\n",
    "#\"max_features\": [auto\", \"sqrt\", \"log2\"],\n",
    "#\"min_samples_split\": [2, 3, 10],\n",
    "# \"min_samples_leaf\": [1, 3, 10],\n",
    "# \"learning_rate\": [0.1, 0.3, 0.5],\n",
    "# \"criterion\": [\"friedman_mse\", \"mse\", \"mae\"],\n",
    "#\"loss\":[\"ls\", \"lad\", \"huber\", \"quantile\"],\n",
    "# \"n_estimators\":[100,200,300]\n",
    "#\n",
    "\n",
    "\n",
    "#use grid over paramaters for GradientBoost Regressor\n",
    "gdr_param_grid = {\"max_depth\": [3, None],\n",
    "                  \"max_features\": [\"auto\", \"sqrt\"],\n",
    "                  \"criterion\": [\"friedman_mse\", \"mse\", \"mae\"],\n",
    "                  \"loss\":[\"ls\", \"lad\"],\n",
    "                  \"n_estimators\":[100,200]}\n",
    "\n",
    "#Implement grid search\n",
    "rfr = RandomForestRegressor(random_state=seed)\n",
    "rfr_gs = GridSearchCV(rfr,rfr_param_grid,cv=num_folds)\n",
    "\n",
    "rfr_gs.fit(X_train, y_train)\n",
    "rfr_gs.best_params_\n",
    "rfr_gs.cv_results_['params']\n",
    "rfr_gs.cv_results_['mean_test_score']\n",
    "\n",
    "#Create visual display of the best estimator\n",
    "mae, mse, rmse, r2score = showTunedModelfit(rfr_gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert Result into DataFrame\n",
    "ml_model = \"Grid-Tuned RandomForest Regressor\"\n",
    "turnedResultsDF = pd.DataFrame({'Model' : [ml_model], 'MAE' : [mae], 'MSE' : [mse],\n",
    "                      'RMSE' : [rmse], 'R2 Score' : [r2score]})\n",
    "turnedResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfr_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for improvement in GradientBoost Regressor\n",
    "\n",
    "ml_model = \"Grid-Tuned GradientBoost Regressor\"\n",
    "gdr = GradientBoostingRegressor(random_state=seed)\n",
    "gdr_gs = GridSearchCV(gdr,gdr_param_grid,cv=num_folds)\n",
    "\n",
    "gdr_gs.fit(X_train, y_train)\n",
    "gdr_gs.best_params_\n",
    "gdr_gs.cv_results_['params']\n",
    "gdr_gs.cv_results_['mean_test_score']\n",
    "\n",
    "#Create visual display of the best estimator\n",
    "mae, mse, rmse, r2score = showTunedModelfit(gdr_gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnedResultsDF.loc[1] = [ml_model, mae, mse, rmse, r2score]\n",
    "turnedResultsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdr_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION - \n",
    "It can be concluded that the top five most important features or properties affecting the compressive strength of Concrete are age, cement, fineagg, coarseagg and water - three of which were among top features also for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamter tuning require access to large computational power, as most of the default setting for most Machine Learning Algorithm seems to have been relatively optimized for good fit and predict capabilities. RandomForest Regressor gave the best result at 93% closely followed by GradientBoosting Regressor at 92.7% putting us within range of the problem statement scope."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
